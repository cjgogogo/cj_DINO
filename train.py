import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter  # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
from tqdm import tqdm
import numpy as np

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—ï¼ˆä¸ä»“åº“ç»“æ„åŒ¹é…ï¼‰
from utils import DefocusDataset, visualize_batch, save_reconstructed_results, denormalize
from models.overall_model import DefocusReconModel
from models.gram_anchoring import GramLoss


def calculate_psnr(pred, target):
    """è®¡ç®—PSNRï¼ˆå³°å€¼ä¿¡å™ªæ¯”ï¼Œè¯„ä¼°é‡å»ºè´¨é‡ï¼‰"""
    mse = nn.MSELoss()(pred, target)
    return 10 * torch.log10(1.0 / mse)  # å‡è®¾åƒç´ å€¼èŒƒå›´[0,1]


def main():
    # ==============================================
    # 1. é…ç½®å‚æ•°ï¼ˆæ ¹æ®ä½ çš„GPUå’Œæ•°æ®é›†è°ƒæ•´ï¼‰
    # ==============================================
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # è®­ç»ƒå‚æ•°
    epochs = 50
    batch_size = 4  # 8Gæ˜¾å­˜å»ºè®®4ï¼Œ16Gæ˜¾å­˜å»ºè®®8
    lr = 1e-4  # åˆå§‹å­¦ä¹ ç‡
    img_size = (512, 512)  # å›¾åƒå°ºå¯¸ï¼ˆéœ€ä¸æ¨¡å‹è¾“å…¥åŒ¹é…ï¼‰
    val_interval = 1  # æ¯å¤šå°‘è½®éªŒè¯ä¸€æ¬¡
    save_interval = 10  # æ¯å¤šå°‘è½®ä¿å­˜ä¸€æ¬¡ä¸­é—´æ¨¡å‹

    # è·¯å¾„é…ç½®
    data_root = "data"
    train_txt = os.path.join(data_root, "train.txt")
    val_txt = os.path.join(data_root, "val.txt")
    log_dir = "logs"  # TensorBoardæ—¥å¿—
    checkpoint_dir = "checkpoints"  # æ¨¡å‹ä¿å­˜ç›®å½•
    result_dir = "results"  # é‡å»ºç»“æœä¿å­˜ç›®å½•

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(checkpoint_dir, exist_ok=True)
    os.makedirs(result_dir, exist_ok=True)

    # åˆå§‹åŒ–TensorBoard
    writer = SummaryWriter(log_dir)

    # ==============================================
    # 2. æ•°æ®åŠ è½½
    # ==============================================
    # è®­ç»ƒé›†ï¼ˆå¯ç”¨æ•°æ®å¢å¼ºï¼‰
    train_dataset = DefocusDataset(
        txt_path=train_txt,
        img_size=img_size,
        is_train=True
    )
    # éªŒè¯é›†ï¼ˆä¸å¯ç”¨å¢å¼ºï¼‰
    val_dataset = DefocusDataset(
        txt_path=val_txt,
        img_size=img_size,
        is_train=False
    )

    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
    if len(train_dataset) == 0:
        raise ValueError(f"è®­ç»ƒé›†ä¸ºç©ºï¼è¯·æ£€æŸ¥ {train_txt} ä¸­çš„è·¯å¾„æ˜¯å¦æ­£ç¡®")
    if len(val_dataset) == 0:
        raise ValueError(f"éªŒè¯é›†ä¸ºç©ºï¼è¯·æ£€æŸ¥ {val_txt} ä¸­çš„è·¯å¾„æ˜¯å¦æ­£ç¡®")

    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,  # å¤šçº¿ç¨‹åŠ è½½
        pin_memory=True  # åŠ é€ŸGPUä¼ è¾“
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2,
        pin_memory=True
    )

    # å¯è§†åŒ–ä¸€ä¸ªè®­ç»ƒbatchï¼ˆè°ƒè¯•ç”¨ï¼‰
    visualize_batch(next(iter(train_loader)), save_path=os.path.join(result_dir, "train_batch_sample.png"))

    # ==============================================
    # 3. æ¨¡å‹åˆå§‹åŒ–ä¸é…ç½®
    # ==============================================
    # åˆå§‹åŒ–ç«¯åˆ°ç«¯æ¨¡å‹
    model = DefocusReconModel().to(device)

    # åŠ è½½DINOv3é¢„è®­ç»ƒæƒé‡ï¼ˆä¿®å¤è·¯å¾„é—®é¢˜ï¼‰
    dinov3_weight_path = "models/dinov3_weights/pytorch_model.bin"
    if os.path.exists(dinov3_weight_path):
        try:
            # ä»…åŠ è½½DINOv3éƒ¨åˆ†æƒé‡ï¼ˆé¿å…ä¸å…¶ä»–æ¨¡å—å†²çªï¼‰
            dinov3_state_dict = torch.load(dinov3_weight_path, map_location=device)
            model.dinov3_feat.model.load_state_dict(dinov3_state_dict, strict=False)
            print("âœ… DINOv3é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ DINOv3æƒé‡åŠ è½½è­¦å‘Šï¼š{e}ï¼ˆç»§ç»­ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼‰")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°DINOv3æƒé‡ {dinov3_weight_path}ï¼ˆç»§ç»­ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼‰")

    # å†»ç»“DINOv3å‰8å±‚ï¼ˆä¿ç•™é¢„è®­ç»ƒç‰¹å¾ï¼Œä»…å¾®è°ƒæ·±å±‚ï¼‰
    for param in list(model.dinov3_feat.model.parameters())[:-4]:
        param.requires_grad = False
    print("âœ… å·²å†»ç»“DINOv3å‰8å±‚å‚æ•°ï¼Œä»…å¾®è°ƒæ·±å±‚")

    # ==============================================
    # 4. æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨
    # ==============================================
    # æ··åˆæŸå¤±ï¼šåƒç´ çº§æŸå¤± + Gramæè´¨ä¸€è‡´æ€§æŸå¤±
    loss_pixel = nn.L1Loss().to(device)  # ä¸»æŸå¤±ï¼šçº¦æŸåƒç´ çº§é‡å»º
    loss_gram = GramLoss().to(device)    # è¾…åŠ©æŸå¤±ï¼šçº¦æŸæè´¨ç»†èŠ‚

    # ä¼˜åŒ–å™¨ï¼ˆAdamWå¸¦æƒé‡è¡°å‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    optimizer = optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),  # ä»…ä¼˜åŒ–å¯è®­ç»ƒå‚æ•°
        lr=lr,
        weight_decay=1e-5
    )

    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼ŒåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼‰
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=epochs,  # é€€ç«å‘¨æœŸ
        eta_min=1e-6   # æœ€å°å­¦ä¹ ç‡
    )

    # ==============================================
    # 5. è®­ç»ƒä¸éªŒè¯å¾ªç¯
    # ==============================================
    best_val_psnr = 0.0  # è®°å½•æœ€ä½³éªŒè¯PSNR

    for epoch in range(1, epochs + 1):
        # ---------------------------
        # è®­ç»ƒé˜¶æ®µ
        # ---------------------------
        model.train()
        train_loss_total = 0.0
        train_psnr_total = 0.0

        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{epochs} [è®­ç»ƒ]")
        for batch_idx, batch in enumerate(pbar):
            # åŠ è½½æ•°æ®
            blur_imgs = batch["blur"].to(device)
            sharp_imgs = batch["sharp"].to(device)

            # å‰å‘ä¼ æ’­ï¼ˆå«æ·±åº¦æ©ç ä¸æ³¨æ„åŠ›æœºåˆ¶ï¼‰
            pred_imgs = model(blur_imgs)

            # è®¡ç®—æŸå¤±
            loss_p = loss_pixel(pred_imgs, sharp_imgs)
            loss_g = loss_gram(pred_imgs, sharp_imgs)
            total_loss = loss_p + 0.1 * loss_g  # GramæŸå¤±æƒé‡0.1ï¼ˆå¯å¾®è°ƒï¼‰

            # åå‘ä¼ æ’­ä¸ä¼˜åŒ–
            optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
            total_loss.backward()  # åå‘ä¼ æ’­
            optimizer.step()       # æ›´æ–°å‚æ•°

            # è®¡ç®—PSNRï¼ˆè®­ç»ƒé›†ï¼‰
            psnr = calculate_psnr(pred_imgs, sharp_imgs)

            # ç´¯è®¡æŸå¤±ä¸PSNR
            train_loss_total += total_loss.item() * blur_imgs.size(0)
            train_psnr_total += psnr.item() * blur_imgs.size(0)

            # å®æ—¶æ˜¾ç¤ºè¿›åº¦
            pbar.set_postfix({
                "L1æŸå¤±": f"{loss_p.item():.4f}",
                "GramæŸå¤±": f"{loss_g.item():.4f}",
                "PSNR": f"{psnr.item():.2f} dB"
            })

            # æ¯100ä¸ªbatchè®°å½•ä¸€æ¬¡è®­ç»ƒå›¾åƒï¼ˆTensorBoardï¼‰
            if batch_idx % 100 == 0:
                writer.add_images(
                    "Train/Blur", denormalize(blur_imgs[:4]),  # å–å‰4å¼ 
                    global_step=epoch * len(train_loader) + batch_idx
                )
                writer.add_images(
                    "Train/Reconstructed", denormalize(pred_imgs[:4]),
                    global_step=epoch * len(train_loader) + batch_idx
                )

        # è®¡ç®—è®­ç»ƒé›†å¹³å‡æŸå¤±ä¸PSNR
        train_loss_avg = train_loss_total / len(train_dataset)
        train_psnr_avg = train_psnr_total / len(train_dataset)
        print(f"\nEpoch {epoch} | è®­ç»ƒé›†å¹³å‡æŸå¤±: {train_loss_avg:.4f} | å¹³å‡PSNR: {train_psnr_avg:.2f} dB")

        # è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°TensorBoard
        writer.add_scalar("Loss/Train", train_loss_avg, epoch)
        writer.add_scalar("PSNR/Train", train_psnr_avg, epoch)
        writer.add_scalar("Learning Rate", optimizer.param_groups[0]["lr"], epoch)

        # ---------------------------
        # éªŒè¯é˜¶æ®µï¼ˆæ¯val_intervalè½®ï¼‰
        # ---------------------------
        if epoch % val_interval == 0:
            model.eval()
            val_loss_total = 0.0
            val_psnr_total = 0.0

            with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—
                pbar_val = tqdm(val_loader, desc=f"Epoch {epoch}/{epochs} [éªŒè¯]")
                for batch_idx, batch in enumerate(pbar_val):
                    blur_imgs = batch["blur"].to(device)
                    sharp_imgs = batch["sharp"].to(device)

                    # å‰å‘ä¼ æ’­
                    pred_imgs = model(blur_imgs)

                    # è®¡ç®—æŸå¤±ä¸PSNR
                    loss_p = loss_pixel(pred_imgs, sharp_imgs)
                    loss_g = loss_gram(pred_imgs, sharp_imgs)
                    total_loss = loss_p + 0.1 * loss_g
                    psnr = calculate_psnr(pred_imgs, sharp_imgs)

                    # ç´¯è®¡æŒ‡æ ‡
                    val_loss_total += total_loss.item() * blur_imgs.size(0)
                    val_psnr_total += psnr.item() * blur_imgs.size(0)

                    # å®æ—¶æ˜¾ç¤ºè¿›åº¦
                    pbar_val.set_postfix({"éªŒè¯PSNR": f"{psnr.item():.2f} dB"})

                    # ä¿å­˜å‰5ä¸ªéªŒè¯æ ·æœ¬çš„é‡å»ºç»“æœ
                    if batch_idx == 0:
                        for i in range(min(5, blur_imgs.size(0))):
                            save_reconstructed_results(
                                recon_img=pred_imgs[i],
                                sharp_img=sharp_imgs[i],
                                blur_img=blur_imgs[i],
                                save_dir=os.path.join(result_dir, f"epoch_{epoch}"),
                                idx=i
                            )

            # è®¡ç®—éªŒè¯é›†å¹³å‡æŒ‡æ ‡
            val_loss_avg = val_loss_total / len(val_dataset)
            val_psnr_avg = val_psnr_total / len(val_dataset)
            print(f"Epoch {epoch} | éªŒè¯é›†å¹³å‡æŸå¤±: {val_loss_avg:.4f} | å¹³å‡PSNR: {val_psnr_avg:.2f} dB")

            # è®°å½•éªŒè¯æŒ‡æ ‡åˆ°TensorBoard
            writer.add_scalar("Loss/Val", val_loss_avg, epoch)
            writer.add_scalar("PSNR/Val", val_psnr_avg, epoch)
            writer.add_images(
                "Val/Reconstructed", denormalize(pred_imgs[:4]),  # å–å‰4å¼ 
                global_step=epoch
            )

            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆæŒ‰éªŒè¯PSNRï¼‰
            if val_psnr_avg > best_val_psnr:
                best_val_psnr = val_psnr_avg
                torch.save({
                    "epoch": epoch,
                    "model_state_dict": model.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "best_psnr": best_val_psnr
                }, os.path.join(checkpoint_dir, "best_model.pth"))
                print(f"ğŸ“Œ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆPSNR: {best_val_psnr:.2f} dBï¼‰")

        # ---------------------------
        # å…¶ä»–æ“ä½œ
        # ---------------------------
        # ä¿å­˜ä¸­é—´æ¨¡å‹
        if epoch % save_interval == 0:
            torch.save(model.state_dict(), os.path.join(checkpoint_dir, f"model_epoch_{epoch}.pth"))
            print(f"ğŸ’¾ ä¿å­˜ä¸­é—´æ¨¡å‹ï¼ˆepoch {epoch}ï¼‰")

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()

    # è®­ç»ƒç»“æŸ
    print(f"\nè®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯PSNR: {best_val_psnr:.2f} dBï¼ˆæ¨¡å‹ä¿å­˜äº {checkpoint_dir}ï¼‰")
    writer.close()


if __name__ == "__main__":
    main()